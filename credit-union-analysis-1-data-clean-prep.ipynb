{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cc97b-93d8-49d2-8f5e-1ce2dfa56856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e41cfa-f99b-412b-b7fb-24ae2dfacd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by reading the 'FederallyInsuredCreditUnions_2024q1.csv' file into a pandas dataframe \n",
    "\n",
    "df = pd.read_csv('your_path/FederallyInsuredCreditUnions_2024q1.csv')\n",
    "\n",
    "# Then get an idea of the size and shape of the dataframe\n",
    "df_shape_initial = print(f\"Initial dataframe is {df.shape[1]} columns wide and {df.shape[0]} rows deep.\")\n",
    "df_shape_initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbe3b9-65d5-41f1-846d-9d4f64f8c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column names along with their indexes for future reference. \n",
    "\n",
    "# Note: some column names break into a second line due to the presence of newline characters ('\\n'), so we'll remove those characters first, making the column names more readable.\n",
    "\n",
    "df.columns = [col.replace('\\n', ' ') for col in df.columns]\n",
    "for idx, col in enumerate(df.columns):\n",
    "    print(f\"{idx}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf09cfd-896e-4884-aff3-a6ac57995874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top and bottom 5 rows to get a sense for the type of data, its completeness, and overall composition.\n",
    "\n",
    "# We'll need to adjust Jupyter's display settings to see all 26  first.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Then we can use 'display' to generate a neat and readable output table.\n",
    "top_bottom = pd.concat([df.head(5), df.tail(5)])\n",
    "display(top_bottom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3584c-2d2c-4fbd-9023-e7e50da8f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on output, we'll adress the following before proceeding with the analysis. \n",
    "# Drop the bottom two rows of containing unecessary information.\n",
    "# Explore the second-to-last column and possibly drop if populated with all nulls.\n",
    "# Correct data types. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cfb07-ca6c-445a-97b9-b03e8d4d3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop Bottom Two Rows\n",
    "\n",
    "# Drop the rows and then have another look at the bottom 5 rows to confirm.\n",
    "df.drop(df.tail(2).index, inplace=True)\n",
    "display(df.tail(5))\n",
    "\n",
    "# We see the last row is at index 4571, \"SESLOC\" CU, and the two records at indexes 4572 and 4573 have been dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d08b6-3cdf-40ba-ae1f-fba7a1dc3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop Null Column\n",
    "# I suspect the second-to-last column has only null values. \n",
    "\n",
    "#We'll first confirm if that's the case, along with a descriptive 'print' command for easily readable output.\n",
    "\n",
    "all_nulls = df.iloc[:, 24].isnull().all()\n",
    "column_name = df.columns[24]\n",
    "print(f\"Are all values in the column '{column_name}' null?: {all_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4df72c-ada2-44c1-a484-a10d8ba28abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing they're all nulls, I'll drop the whole column at index 24 and take another look at the bottom rows.\n",
    "df = df.drop(df.columns[24], axis=1)\n",
    "df_shape_new = print(f\"New dataframe is {df.shape[1]} columns wide and {df.shape[0]} rows deep.\\n\")\n",
    "df_shape_new\n",
    "display(df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127373a-aa0f-432e-a15c-c9fb51408d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Correcting Data Types\n",
    "\n",
    "# Trying to correct the data types with NaN values in the dataframe can be complicated. \n",
    "\n",
    "# I'll cover properly addressing null values a bit later. But to move ahead with correcting data types now, I'm going to use a three-step approach.\n",
    "# First, create a temporary df where I'll convert the null values to non-null values.\n",
    "# Second, convert the data types. \n",
    "# Third, replace the nulls so we can address them properly a bit later.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f080bd5-18f3-40d2-aff5-1800571e1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Temporarily Replace Nulls\n",
    "\n",
    "# Let's start by viewing the current data types\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7313870-d3da-46c7-8980-908271a04ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice discrepancies in data types, e.g.,'Year' should not be a float64 dtype, and 'Members' should not be an object dtype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac32c12-0100-4e80-b1a5-f2e0947e079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a. Create a Copy of the Dataframe\n",
    "\n",
    "# A copy of the dataframe allows us to convert null values while still preserving them in the original dataframe. \n",
    "df_temp = df.copy()\n",
    "\n",
    "# Print column names to ensure we enter them exactly as they're presented in the dataframe (e.g. characters, whitespace, etc.). \n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af1989-9a18-460c-bcc7-f05709963c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b. Convert Null Values in Temp Dataframe\n",
    "\n",
    "# Replace null values in numeric columns of the temp dataframe with 0.\n",
    "\n",
    "# Let's declare and initialize a resuable variable for numeric columns\n",
    "numeric_columns = ['Charter number', 'Members', 'Total assets', 'Total loans',\n",
    "                   'Total deposits', 'Total deposits, 4 quarter growth  (%)',\n",
    "                   'Total loans,  4 quarter growth  (%)', 'Total assets,  4 quarter growth  (%)',\n",
    "                   'Members, 4 quarter growth  (%)', 'Net worth,  4 quarter growth (excludes CECL transition provision) (%)']\n",
    "\n",
    "# Now replace invalid values with 0 in the temp dataframe. Note: We'll need to replace several invalid values such as 'NA', 'N/M', etc., in addition to nulls.\n",
    "for col in df_temp:\n",
    "    df_temp[col] = df_temp[col].replace(['NA','N/M', 'N/M - Not Meaningful','N/M - Not Meaningful ', np.nan], 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9986e3-97e1-43d7-8e80-04b8c6104098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify there are no null values in any records.\n",
    "\n",
    "null_indexes = df_temp[df_temp.isnull().any(axis=1)].index.tolist()\n",
    "print(\"Indexes of rows with null values:\", null_indexes)\n",
    "\n",
    "# Having converted all the null values in the temporary dataframe, we can now correct our data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9063f7-f389-452b-999c-14cbe4cd0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Correct the Data Types in Temp Dataframe\n",
    "\n",
    "# I'll create a reusable function that takes a parameter 'data' which can be any dataframe with the same structure, and performs the required data type conversions.\n",
    "\n",
    "def convert_dataframe_types(data):\n",
    "    # Convert columns to int\n",
    "    int_columns = [\n",
    "        'Year', 'Quarter ', 'NCUA region', 'Charter number', \n",
    "        'NCUA internal ID (join_number)'\n",
    "    ]\n",
    "    for col in int_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].astype(int)\n",
    "    \n",
    "    # Convert columns to float\n",
    "    float_columns = [\n",
    "        'Total deposits, 4 quarter growth  (%)', \n",
    "        'Total loans,  4 quarter growth  (%)', \n",
    "        'Total assets,  4 quarter growth  (%)', \n",
    "        'Members, 4 quarter growth  (%)', \n",
    "        'Net worth,  4 quarter growth (excludes CECL transition provision) (%)'\n",
    "    ]\n",
    "    for col in float_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].astype(float)\n",
    "    \n",
    "    # Convert Zip code to 5-digit string\n",
    "    zip_code_column = 'Zip code (Mailing address)'\n",
    "    if zip_code_column in data.columns:\n",
    "        data[zip_code_column] = data[zip_code_column].replace([np.nan], 0).astype(int).astype(str).str.zfill(5)\n",
    "    \n",
    "    # Remove commas and convert to int\n",
    "    comma_int_columns = ['Members', 'Total assets', 'Total loans', 'Total deposits']\n",
    "    for col in comma_int_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].str.replace(',', '').astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Usage with df_temp\n",
    "df_temp = convert_dataframe_types(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2767d56-ea1b-4a87-869b-96a615970695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data types have been properly updated.\n",
    "print(df_temp.dtypes)\n",
    "display(df_temp.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb1b47-0267-4969-b5e1-1381c306042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Decide How to Handle Null Values in Original Dataframe\n",
    "\n",
    "# If none of the null-containing records belong to the top 5 credit unions as measured by 'Members', 'Total assets', 'Total loans', or 'Total deposits', we will drop them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8197a-30bc-494b-98ad-7f4234b6606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a. Get a list of records with null values\n",
    "\n",
    "# Create a variable containing the indexes of rows with null values in the original dataframe. \n",
    "null_indexes_df = df[df.isnull().any(axis=1)].index\n",
    "print(\"Indexes of rows with null values in original df:\", null_indexes_df.tolist())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38852f6b-0ba2-467b-8cfc-d01609667ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Use the corresponding null-free records in the temp dataframe to decide appropriate resolution\n",
    "\n",
    "# We first have to define our target groups so we can then check if they include any records from null_indexes_df.\n",
    "\n",
    "# Assign variables for the top 5 groups.\n",
    "top_5_members = df_temp.nlargest(5, 'Members').index\n",
    "top_5_assets = df_temp.nlargest(5, 'Total assets').index\n",
    "top_5_loans = df_temp.nlargest(5, 'Total loans').index\n",
    "top_5_deposits = df_temp.nlargest(5, 'Total deposits').index\n",
    "\n",
    "# Combine the \"top 5\" indexes.\n",
    "top_indexes = set(top_5_members).union(set(top_5_assets)).union(set(top_5_loans)).union(set(top_5_deposits))\n",
    "\n",
    "# Assign a variable to check for intersections between null_indexes_df and top_indexes.\n",
    "impactful_null_rows = set(null_indexes_df).intersection(top_indexes)\n",
    "\n",
    "# Then check to see if there are any impactful null records.\n",
    "if impactful_null_rows:\n",
    "    print(\"These rows with null values are among the top 5 in Members, Total assets, or Total loans:\", impactful_null_rows)\n",
    "else:\n",
    "    print(\"No rows with null values are among the top 5 in Members, Total assets, or Total loans.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c43e76-1571-4193-b10d-5452e1565c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output, we can safely drop the null-containing records in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f21b87-b41d-4573-8567-22ccc1b1233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two important steps we performed on the temporary dataframe, which we now need to perform on the original dataframe.\n",
    "# Those are: dropping the null-containing rows and correcting the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eddf08-5865-4158-8ec2-d3158c06e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we drop the non-impactful null rows.\n",
    "if not impactful_null_rows:\n",
    "    df = df.drop(null_indexes_df)\n",
    "    print(\"Dropped rows with null values. Remaining rows:\", df.shape[0])\n",
    "else:\n",
    "    print(\"Consider handling the nulls in the identified top rows instead of dropping.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1da37-f1fc-44d0-997a-17df28a04357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 11 records have been dropped leaving us with 4561 rows in the original dataframe vs the original 4572.\n",
    "\n",
    "# We'll now convert the data types in the original dataframe.\n",
    "\n",
    "# First confirm the current data types and format of the dataframe.\n",
    "print(df.dtypes)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479bc44-965a-4fda-8307-530097aa64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice some data type discrepancies, e.g., 'Year' as float64, and Zip codes as float64.\n",
    "\n",
    "# Use the convert_dataframe_types(data) function we created earlier and pass 'df' as the parameter.\n",
    "# Executing the function and looking once again at the data types and dataframe confirms that it's been done.\n",
    "convert_dataframe_types(df)\n",
    "print(df.dtypes)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb3d80-5769-4690-a7c5-1aa0532dd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This concludes the initial cleanup of our dataframe. We'll have a much easier time conducting further analysis by effectively having removed potential issues and complexity, and gained familiarity with the structure and contents of the dataframe.\n",
    "# We'll save this df as a csv so we can easily reference or share it later.\n",
    "\n",
    "df.to_csv('cleaned_credit_union_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
